{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "787b8fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\".\")\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fa33bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.image_utils import *\n",
    "from model.my_diffusers.models import AutoencoderKL_Pretrained \n",
    "import trimesh\n",
    "import os \n",
    "import torch.nn.functional as F\n",
    "from typing import Sequence, Union\n",
    "\n",
    "os.environ[\"OPENCV_IO_ENABLE_OPENEXR\"]=\"1\"\n",
    "\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "695a2fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def np2th(ndarray):\n",
    "    if isinstance(ndarray, torch.Tensor):\n",
    "        return ndarray.detach().cpu()\n",
    "    elif isinstance(ndarray, np.ndarray):\n",
    "        return torch.tensor(ndarray).float()\n",
    "    else:\n",
    "        raise ValueError(\"Input should be either torch.Tensor or np.ndarray\")\n",
    "\n",
    "def th2np(tensor):\n",
    "    if isinstance(tensor, np.ndarray):\n",
    "        return tensor\n",
    "    if isinstance(tensor, torch.Tensor):\n",
    "        return tensor.detach().cpu().numpy()\n",
    "\n",
    "        \n",
    "def normalize_points(p, method: str=\"sphere\"):\n",
    "    if method == \"sphere\":\n",
    "        return _to_unit_sphere(p)\n",
    "    elif method == \"cube\":\n",
    "        return _to_unit_cube(p)\n",
    "    else:\n",
    "        raise AssertionError\n",
    "\n",
    "def _to_unit_sphere(pc: Union[np.ndarray, torch.Tensor]):\n",
    "    \"\"\"\n",
    "    pc: [B,N,3] or [N,3]\n",
    "    \"\"\"\n",
    "    dtype = type(pc)\n",
    "    pc = np2th(pc)\n",
    "    shapes = pc.shape\n",
    "    N = shapes[-2]\n",
    "    pc = pc.reshape(-1, N, 3)\n",
    "    m = pc.mean(1, keepdim=True)\n",
    "    pc = pc - m\n",
    "    s = torch.max(torch.sqrt(torch.sum(pc**2, -1, keepdim=True)), 1, keepdim=True)[0]\n",
    "    pc = pc / s\n",
    "    pc = pc.reshape(shapes)\n",
    "    if dtype == np.ndarray:\n",
    "        return th2np(pc)\n",
    "    return pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0eee5236",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:5') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b031993",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import time\n",
    "import traceback\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "import numpy\n",
    "import skimage.io\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import Imath\n",
    "import OpenEXR\n",
    "\n",
    "\n",
    "class Warper:\n",
    "    def __init__(self, resolution: tuple = None, device: str = 'gpu0'):\n",
    "        self.resolution = resolution\n",
    "        self.device = self.get_device(device)\n",
    "        return\n",
    "\n",
    "    def forward_warp(self, \n",
    "                     frame1: torch.Tensor, \n",
    "                     mask1: Optional[torch.Tensor], \n",
    "                     depth1: torch.Tensor,\n",
    "                     transformation1: torch.Tensor, \n",
    "                     transformation2: torch.Tensor, \n",
    "                     intrinsic1: torch.Tensor, \n",
    "                     intrinsic2: Optional[torch.Tensor],\n",
    "                     render_method: str = \"splatting\") -> \\\n",
    "            Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \n",
    "        \"\"\"\n",
    "         Given a frame1 and global transformations transformation1 and transformation2, \n",
    "         warps frame1 to next view using bilinear splatting.\n",
    "        \n",
    "         All arrays should be torch tensors with batch dimension and channel first\n",
    "         :param frame1: (b, 3, h, w). If frame1 is not in the range [-1, 1], either set is_image=False when calling\n",
    "                        bilinear_splatting on frame within this function, or modify clipping in bilinear_splatting()\n",
    "                        method accordingly.\n",
    "         :param mask1: (b, 1, h, w) - 1 for known, 0 for unknown. Optional\n",
    "         :param depth1: (b, 1, h, w)\n",
    "         :param transformation1: (b, 4, 4) extrinsic transformation matrix of first view: [R, t; 0, 1]\n",
    "         :param transformation2: (b, 4, 4) extrinsic transformation matrix of second view: [R, t; 0, 1]\n",
    "         :param intrinsic1: (b, 3, 3) camera intrinsic matrix\n",
    "         :param intrinsic2: (b, 3, 3) camera intrinsic matrix. Optional\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.resolution is not None:\n",
    "            assert frame1.shape[2:4] == self.resolution\n",
    "        b, c, h, w = frame1.shape\n",
    "        if mask1 is None:\n",
    "            mask1 = torch.ones(size=(b, 1, h, w)).to(frame1)\n",
    "        if intrinsic2 is None:\n",
    "            intrinsic2 = intrinsic1.clone()\n",
    "        \n",
    "        assert frame1.shape == (b, 3, h, w)\n",
    "        assert mask1.shape == (b, 1, h, w)\n",
    "        assert depth1.shape == (b, 1, h, w)\n",
    "        assert transformation1.shape == (b, 4, 4)\n",
    "        assert transformation2.shape == (b, 4, 4)\n",
    "        assert intrinsic1.shape == (b, 3, 3)\n",
    "        assert intrinsic2.shape == (b, 3, 3)\n",
    "\n",
    "        frame1 = frame1.to(self.device)\n",
    "        mask1 = mask1.to(self.device)\n",
    "        depth1 = depth1.to(self.device)\n",
    "        transformation1 = transformation1.to(self.device)\n",
    "        transformation2 = transformation2.to(self.device)\n",
    "        intrinsic1 = intrinsic1.to(self.device)\n",
    "        intrinsic2 = intrinsic2.to(self.device)\n",
    "\n",
    "        # Backproject source grid into target grid\n",
    "        # trans_points1 (b, h, w, 3, 1) holds warped 2D homogeneous coordinates\n",
    "        trans_points1 = self.compute_transformed_points(depth1,\n",
    "                                                        transformation1,\n",
    "                                                        transformation2,\n",
    "                                                        intrinsic1,\n",
    "                                                        intrinsic2,\n",
    "                                                        normalized_coordinates=False)\n",
    "        \n",
    "        # Homogeneous coordinates to u, v coordinates\n",
    "        # Jaihoon\n",
    "        trans_coordinates = trans_points1[:, :, :, :2, 0] / trans_points1[:, :, :, 2:3, 0]\n",
    "        # Jaihoon \n",
    "#         print(\"ndc - trans_coordinates\", trans_coordinates.shape, trans_coordinates[0][400][400], trans_coordinates[0][450][450], trans_coordinates[0][450][400])\n",
    "#         trans_coordinates = self.unnormalize(b, h, w, trans_coordinates)\n",
    "        print(\"uv space - trans_coordinates\", trans_coordinates.shape, trans_coordinates[0][400][400], trans_coordinates[0][450][450], trans_coordinates[0][450][400])\n",
    "        \n",
    "        trans_depth1 = trans_points1[:, :, :, 2, 0]\n",
    "        \"\"\"\n",
    "        grid format\n",
    "            [[[0, 0], [1, 0], ...],\n",
    "             [[1, 0], [1, 1], ...]] : [x, y]\n",
    "        \"\"\"\n",
    "        # 2023.07.27 Jaihoon\n",
    "        grid = self.create_grid(b, h, w, normalized_coordinates=False).to(trans_coordinates)\n",
    "        grid = grid.permute(0, 2, 3, 1)\n",
    "        \n",
    "        # Source view -> Target view displacement \n",
    "        flow12 = trans_coordinates - grid\n",
    "        flow12 = flow12.permute(0, 3, 1, 2)         # 2023.07.27 Jaihoon\n",
    "        \n",
    "        if render_method == \"splatting\":\n",
    "            print(f\"rendering method splatting\")\n",
    "            warped_frame2, mask2 = self.bilinear_splatting(frame1, \n",
    "                                                           mask1, \n",
    "                                                           trans_depth1, \n",
    "                                                           flow12, \n",
    "                                                           None, \n",
    "                                                           is_image=True)\n",
    "        elif render_method == \"interpolation\":\n",
    "            print(f\"rendering method interpolation\")\n",
    "            warped_frame2, mask2 = self.bilinear_interpolation(frame1, \n",
    "                                                               mask1, \n",
    "                                                               flow12,\n",
    "                                                               flow12_mask=None,\n",
    "                                                               is_image=True)\n",
    "        elif render_method == \"nearest\":\n",
    "            raise NotImplementedError(f\"Not implemented method {render_method}\")\n",
    "            print(f\"rendering method nearest\")\n",
    "            warped_frame2 = self.nearest_interpolation(frame1, \n",
    "                                                        mask1, \n",
    "                                                        flow12,\n",
    "                                                        flow12_mask=None,\n",
    "                                                        is_image=True)\n",
    "        \n",
    "        else:\n",
    "            raise NotImplementedError(f\"Not implemented method {render_method}\")\n",
    "        \n",
    "        warped_depth2 = self.bilinear_splatting(trans_depth1[:, :, None], \n",
    "                                                mask1, \n",
    "                                                trans_depth1, \n",
    "                                                flow12, \n",
    "                                                None,\n",
    "                                                is_image=False)[0][:, :, 0]\n",
    "        \n",
    "        return warped_frame2, mask2, warped_depth2, flow12\n",
    "    \n",
    "    \n",
    "    def unnormalize(self, b, h, w, tensor):\n",
    "        tensor[..., 0] = (tensor[..., 0] + 1) * (w-1) / 2\n",
    "        tensor[..., 1] = (tensor[..., 1] + 1) * (h-1) / 2\n",
    "        \n",
    "        return tensor\n",
    "    \n",
    "\n",
    "    def compute_transformed_points(self, \n",
    "                                   depth1: torch.Tensor, \n",
    "                                   transformation1: torch.Tensor, \n",
    "                                   transformation2: torch.Tensor,\n",
    "                                   intrinsic1: torch.Tensor, \n",
    "                                   intrinsic2: Optional[torch.Tensor],\n",
    "                                   normalized_coordinates: bool = False):\n",
    "        \n",
    "        \"\"\"\n",
    "        Computes transformed position for each pixel location\n",
    "        :param depth1: (b, 1, h, w)\n",
    "        :param transformation1: (b, 4, 4)\n",
    "        :param transformation2: (b, 4, 4)\n",
    "        :param intrinsic1: (b, 3, 3)\n",
    "        :param intrinsic2: (b, 3, 3)\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.resolution is not None:\n",
    "            assert depth1.shape[2:4] == self.resolution\n",
    "        b, _, h, w = depth1.shape\n",
    "        if intrinsic2 is None:\n",
    "            intrinsic2 = intrinsic1.clone()\n",
    "        transformation = torch.bmm(transformation2, torch.linalg.inv(transformation1))  # (b, 4, 4)\n",
    "        \n",
    "        if normalized_coordinates:\n",
    "            x1d = torch.linspace(-1, 1, w)[None]\n",
    "            y1d = torch.linspace(-1, 1, h)[:, None]\n",
    "\n",
    "        else:\n",
    "            # Compute source grid points in homogeneous coordinate system\n",
    "            x1d = torch.arange(0, w)[None]\n",
    "            # Jaihoon\n",
    "            y1d = torch.arange(0, h)[:, None]\n",
    "    #         y1d = -torch.arange(0, h)[:, None]\n",
    "    \n",
    "        x2d = x1d.repeat([h, 1]).to(depth1)  # (h, w)\n",
    "        y2d = y1d.repeat([1, w]).to(depth1)  # (h, w)\n",
    "\n",
    "        ones_2d = torch.ones(size=(h, w)).to(depth1)  # (h, w)\n",
    "        ones_4d = ones_2d[None, :, :, None, None].repeat([b, 1, 1, 1, 1])  # (b, h, w, 1, 1)\n",
    "        # Jaihoon\n",
    "        pos_vectors_homo = torch.stack([x2d, y2d, ones_2d], dim=2)[None, :, :, :, None]  # (1, h, w, 3, 1)\n",
    "#         pos_vectors_homo = torch.stack([x2d, y2d, -ones_2d], dim=2)[None, :, :, :, None]  # (1, h, w, 3, 1)\n",
    "\n",
    "        print(\"pos_vectors_homo\", pos_vectors_homo.shape, pos_vectors_homo[0][400][400], pos_vectors_homo[0][450][450], pos_vectors_homo[0][450][400])\n",
    "        \n",
    "        # Compute projective transformation from source to target view\n",
    "        intrinsic1_inv = torch.linalg.inv(intrinsic1)  # (b, 3, 3)\n",
    "        intrinsic1_inv_4d = intrinsic1_inv[:, None, None]  # (b, 1, 1, 3, 3)\n",
    "        intrinsic2_4d = intrinsic2[:, None, None]  # (b, 1, 1, 3, 3)\n",
    "        depth_4d = depth1[:, 0][:, :, :, None, None]  # (b, h, w, 1, 1)\n",
    "        trans_4d = transformation[:, None, None]  # (b, 1, 1, 4, 4)\n",
    "        \n",
    "        # Warp source grid into target grid, return warped 2D homogeneous coordinates\n",
    "        unnormalized_pos = torch.matmul(intrinsic1_inv_4d, pos_vectors_homo)  # (b, h, w, 3, 1)\n",
    "        print(\"unnormalized_pos\", unnormalized_pos.shape, unnormalized_pos[0][400][400], unnormalized_pos[0][450][450], unnormalized_pos[0][450][400])\n",
    "        world_points = depth_4d * unnormalized_pos  # (b, h, w, 3, 1)\n",
    "        ndc_transformation = torch.zeros\n",
    "        \n",
    "        foreground_mask = depth_4d != torch.max(depth_4d)\n",
    "        foreground_mask = torch.cat([foreground_mask, foreground_mask, foreground_mask], dim=-2)\n",
    "        print(\"world_points center\", torch.mean(world_points[foreground_mask].reshape(-1, 3), dim=0))\n",
    "\n",
    "        # Jaihoon \n",
    "#         if normalized_coordinates:\n",
    "#             print(\"world_points center\", torch.mean(world_points[foreground_mask].reshape(-1, 3), dim=0))\n",
    "#             print(\"world_points[foreground_mask]\", world_points[foreground_mask].shape)\n",
    "#             print(\"foreground_mask\", foreground_mask.shape)\n",
    "#             print(\"normalize_points(world_points[foreground_mask].reshape(-1, 3)).flatten()\", normalize_points(world_points[foreground_mask].reshape(-1, 3)).flatten().shape)\n",
    "#             world_points[foreground_mask] = normalize_points(world_points[foreground_mask].reshape(-1, 3)).flatten().to(world_points)\n",
    "        \n",
    "\n",
    "        world_points_homo = torch.cat([world_points, ones_4d], dim=3)  # (b, h, w, 4, 1)\n",
    "        print(\"world_points_homo\", world_points_homo.shape, world_points_homo[0][400][400], world_points_homo[0][450][450], world_points_homo[0][450][400])\n",
    "        print(\"T2 T1.inv\", trans_4d[0][0][0])\n",
    "        trans_world_homo = torch.matmul(trans_4d, world_points_homo)  # (b, h, w, 4, 1)\n",
    "        print(\"trans_world_homo\", trans_world_homo[0][400][400], trans_world_homo[0][450][450], trans_world_homo[0][450][400])\n",
    "        \n",
    "        trans_world = trans_world_homo[:, :, :, :3]  # (b, h, w, 3, 1)\n",
    "        trans_norm_points = torch.matmul(intrinsic2_4d, trans_world)  # (b, h, w, 3, 1)\n",
    "        print(\"trans_norm_points\", trans_norm_points[0][400][400], trans_norm_points[0][450][450], trans_norm_points[0][450][400])\n",
    "        \n",
    "        return trans_norm_points\n",
    "\n",
    "    \n",
    "    def bilinear_splatting(self, \n",
    "                           frame1: torch.Tensor, \n",
    "                           mask1: Optional[torch.Tensor], \n",
    "                           depth1: torch.Tensor,\n",
    "                           flow12: torch.Tensor, \n",
    "                           flow12_mask: Optional[torch.Tensor], \n",
    "                           is_image: bool = False) -> \\\n",
    "            Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \n",
    "        \"\"\"\n",
    "        Bilinear splatting\n",
    "        :param frame1: (b,c,h,w)\n",
    "        :param mask1: (b,1,h,w): 1 for known, 0 for unknown. Optional\n",
    "        :param depth1: (b,1,h,w)\n",
    "        :param flow12: (b,2,h,w)\n",
    "        :param flow12_mask: (b,1,h,w): 1 for valid flow, 0 for invalid flow. Optional\n",
    "        :param is_image: if true, output will be clipped to (-1,1) range\n",
    "        :return: warped_frame2: (b,c,h,w)\n",
    "                 mask2: (b,1,h,w): 1 for known and 0 for unknown\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.resolution is not None:\n",
    "            assert frame1.shape[2:4] == self.resolution\n",
    "        b, c, h, w = frame1.shape\n",
    "        \n",
    "        if mask1 is None:\n",
    "            mask1 = torch.ones(size=(b, 1, h, w)).to(frame1)\n",
    "            \n",
    "        if flow12_mask is None:\n",
    "            flow12_mask = torch.ones(size=(b, 1, h, w)).to(flow12)\n",
    "        \n",
    "        # Compute relative weight to the closest grid \n",
    "        grid = self.create_grid(b, h, w).to(frame1)\n",
    "        trans_pos = flow12 + grid\n",
    "\n",
    "        trans_pos_offset = trans_pos + 1\n",
    "        trans_pos_floor = torch.floor(trans_pos_offset).long()\n",
    "        trans_pos_ceil = torch.ceil(trans_pos_offset).long()\n",
    "        \n",
    "        trans_pos_offset = torch.stack([\n",
    "            torch.clamp(trans_pos_offset[:, 0], min=0, max=w + 1),\n",
    "            torch.clamp(trans_pos_offset[:, 1], min=0, max=h + 1)], dim=1)\n",
    "        \n",
    "        trans_pos_floor = torch.stack([\n",
    "            torch.clamp(trans_pos_floor[:, 0], min=0, max=w + 1),\n",
    "            torch.clamp(trans_pos_floor[:, 1], min=0, max=h + 1)], dim=1)\n",
    "        \n",
    "        trans_pos_ceil = torch.stack([\n",
    "            torch.clamp(trans_pos_ceil[:, 0], min=0, max=w + 1),\n",
    "            torch.clamp(trans_pos_ceil[:, 1], min=0, max=h + 1)], dim=1)\n",
    "        \n",
    "        prox_weight_nw = (1 - (trans_pos_offset[:, 1:2] - trans_pos_floor[:, 1:2])) * \\\n",
    "                         (1 - (trans_pos_offset[:, 0:1] - trans_pos_floor[:, 0:1]))\n",
    "        \n",
    "        prox_weight_sw = (1 - (trans_pos_ceil[:, 1:2] - trans_pos_offset[:, 1:2])) * \\\n",
    "                         (1 - (trans_pos_offset[:, 0:1] - trans_pos_floor[:, 0:1]))\n",
    "        \n",
    "        prox_weight_ne = (1 - (trans_pos_offset[:, 1:2] - trans_pos_floor[:, 1:2])) * \\\n",
    "                         (1 - (trans_pos_ceil[:, 0:1] - trans_pos_offset[:, 0:1]))\n",
    "        \n",
    "        prox_weight_se = (1 - (trans_pos_ceil[:, 1:2] - trans_pos_offset[:, 1:2])) * \\\n",
    "                         (1 - (trans_pos_ceil[:, 0:1] - trans_pos_offset[:, 0:1]))\n",
    "\n",
    "        sat_depth1 = torch.clamp(depth1, min=0, max=1000)\n",
    "        log_depth1 = torch.log(1 + sat_depth1)\n",
    "        depth_weights = torch.exp(log_depth1 / log_depth1.max() * 60)\n",
    "        \n",
    "        # Attenuate weight by depth value\n",
    "        weight_nw = torch.moveaxis(prox_weight_nw * mask1 * flow12_mask / depth_weights, [0, 1, 2, 3], [0, 3, 1, 2])\n",
    "        weight_sw = torch.moveaxis(prox_weight_sw * mask1 * flow12_mask / depth_weights, [0, 1, 2, 3], [0, 3, 1, 2])\n",
    "        weight_ne = torch.moveaxis(prox_weight_ne * mask1 * flow12_mask / depth_weights, [0, 1, 2, 3], [0, 3, 1, 2])\n",
    "        weight_se = torch.moveaxis(prox_weight_se * mask1 * flow12_mask / depth_weights, [0, 1, 2, 3], [0, 3, 1, 2])\n",
    "\n",
    "        warped_frame = torch.zeros(size=(b, h + 2, w + 2, c), dtype=torch.float32).to(frame1)\n",
    "        warped_weights = torch.zeros(size=(b, h + 2, w + 2, 1), dtype=torch.float32).to(frame1)\n",
    "        \n",
    "        frame1_cl = torch.moveaxis(frame1, [0, 1, 2, 3], [0, 3, 1, 2])\n",
    "        batch_indices = torch.arange(b)[:, None, None].to(frame1.device)\n",
    "                \n",
    "        warped_frame.index_put_((batch_indices,\n",
    "                                 trans_pos_floor[:, 1], \n",
    "                                 trans_pos_floor[:, 0]),\n",
    "                                 frame1_cl * weight_nw, \n",
    "                                 accumulate=True)\n",
    "        \n",
    "        warped_frame.index_put_((batch_indices, \n",
    "                                 trans_pos_ceil[:, 1], \n",
    "                                 trans_pos_floor[:, 0]),\n",
    "                                 frame1_cl * weight_sw, \n",
    "                                 accumulate=True)\n",
    "        \n",
    "        warped_frame.index_put_((batch_indices, \n",
    "                                 trans_pos_floor[:, 1], \n",
    "                                 trans_pos_ceil[:, 0]),\n",
    "                                 frame1_cl * weight_ne, \n",
    "                                 accumulate=True)\n",
    "        \n",
    "        warped_frame.index_put_((batch_indices, \n",
    "                                 trans_pos_ceil[:, 1], \n",
    "                                 trans_pos_ceil[:, 0]),\n",
    "                                 frame1_cl * weight_se, \n",
    "                                 accumulate=True)\n",
    "\n",
    "        warped_weights.index_put_((batch_indices, \n",
    "                                   trans_pos_floor[:, 1], \n",
    "                                   trans_pos_floor[:, 0]),\n",
    "                                   weight_nw, \n",
    "                                   accumulate=True)\n",
    "        \n",
    "        warped_weights.index_put_((batch_indices, \n",
    "                                   trans_pos_ceil[:, 1], \n",
    "                                   trans_pos_floor[:, 0]),\n",
    "                                   weight_sw, \n",
    "                                   accumulate=True)\n",
    "        \n",
    "        warped_weights.index_put_((batch_indices, \n",
    "                                   trans_pos_floor[:, 1], \n",
    "                                   trans_pos_ceil[:, 0]),\n",
    "                                   weight_ne, \n",
    "                                   accumulate=True)\n",
    "        \n",
    "        warped_weights.index_put_((batch_indices, \n",
    "                                   trans_pos_ceil[:, 1], \n",
    "                                   trans_pos_ceil[:, 0]),\n",
    "                                   weight_se,\n",
    "                                   accumulate=True)\n",
    "\n",
    "        warped_frame_cf = torch.moveaxis(warped_frame, [0, 1, 2, 3], [0, 2, 3, 1])\n",
    "        warped_weights_cf = torch.moveaxis(warped_weights, [0, 1, 2, 3], [0, 2, 3, 1])\n",
    "        cropped_warped_frame = warped_frame_cf[:, :, 1:-1, 1:-1]\n",
    "        cropped_weights = warped_weights_cf[:, :, 1:-1, 1:-1]\n",
    "\n",
    "        mask = cropped_weights > 0\n",
    "        zero_value = 0\n",
    "        zero_tensor = torch.tensor(zero_value, dtype=frame1.dtype, device=frame1.device)\n",
    "        warped_frame2 = torch.where(mask, cropped_warped_frame / cropped_weights, zero_tensor)\n",
    "        mask2 = mask.to(frame1)\n",
    "\n",
    "        if is_image:\n",
    "            assert warped_frame2.min() >= -1.1  # Allow for rounding errors\n",
    "            assert warped_frame2.max() <= 1.1\n",
    "            warped_frame2 = torch.clamp(warped_frame2, min=0, max=1)\n",
    "            \n",
    "        return warped_frame2, mask2\n",
    "    \n",
    "\n",
    "    def bilinear_interpolation(self, \n",
    "                               frame2: torch.Tensor, \n",
    "                               mask2: Optional[torch.Tensor], \n",
    "                               flow12: torch.Tensor,\n",
    "                               flow12_mask: Optional[torch.Tensor], \n",
    "                               is_image: bool = False) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Bilinear interpolation\n",
    "        :param frame2: (b, c, h, w)\n",
    "        :param mask2: (b, 1, h, w): 1 for known, 0 for unknown. Optional\n",
    "        :param flow12: (b, 2, h, w)\n",
    "        :param flow12_mask: (b, 1, h, w): 1 for valid flow, 0 for invalid flow. Optional\n",
    "        :param is_image: if true, output will be clipped to (-1,1) range\n",
    "        :return: warped_frame1: (b, c, h, w)\n",
    "                 mask1: (b, 1, h, w): 1 for known and 0 for unknown\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.resolution is not None:\n",
    "            assert frame2.shape[2:4] == self.resolution\n",
    "        b, c, h, w = frame2.shape\n",
    "        \n",
    "        if mask2 is None:\n",
    "            mask2 = torch.ones(size=(b, 1, h, w)).to(frame2)\n",
    "            \n",
    "        if flow12_mask is None:\n",
    "            flow12_mask = torch.ones(size=(b, 1, h, w)).to(flow12)\n",
    "            \n",
    "        grid = self.create_grid(b, h, w).to(frame2)\n",
    "        trans_pos = flow12 + grid\n",
    "\n",
    "        trans_pos_offset = trans_pos + 1\n",
    "        trans_pos_floor = torch.floor(trans_pos_offset).long()\n",
    "        trans_pos_ceil = torch.ceil(trans_pos_offset).long()\n",
    "        \n",
    "        trans_pos_offset = torch.stack([\n",
    "            torch.clamp(trans_pos_offset[:, 0], min=0, max=w + 1),\n",
    "            torch.clamp(trans_pos_offset[:, 1], min=0, max=h + 1)], dim=1)\n",
    "        \n",
    "        trans_pos_floor = torch.stack([\n",
    "            torch.clamp(trans_pos_floor[:, 0], min=0, max=w + 1),\n",
    "            torch.clamp(trans_pos_floor[:, 1], min=0, max=h + 1)], dim=1)\n",
    "        \n",
    "        trans_pos_ceil = torch.stack([\n",
    "            torch.clamp(trans_pos_ceil[:, 0], min=0, max=w + 1),\n",
    "            torch.clamp(trans_pos_ceil[:, 1], min=0, max=h + 1)], dim=1)\n",
    "\n",
    "        prox_weight_nw = (1 - (trans_pos_offset[:, 1:2] - trans_pos_floor[:, 1:2])) * \\\n",
    "                         (1 - (trans_pos_offset[:, 0:1] - trans_pos_floor[:, 0:1]))\n",
    "        prox_weight_sw = (1 - (trans_pos_ceil[:, 1:2] - trans_pos_offset[:, 1:2])) * \\\n",
    "                         (1 - (trans_pos_offset[:, 0:1] - trans_pos_floor[:, 0:1]))\n",
    "        prox_weight_ne = (1 - (trans_pos_offset[:, 1:2] - trans_pos_floor[:, 1:2])) * \\\n",
    "                         (1 - (trans_pos_ceil[:, 0:1] - trans_pos_offset[:, 0:1]))\n",
    "        prox_weight_se = (1 - (trans_pos_ceil[:, 1:2] - trans_pos_offset[:, 1:2])) * \\\n",
    "                         (1 - (trans_pos_ceil[:, 0:1] - trans_pos_offset[:, 0:1]))\n",
    "\n",
    "        weight_nw = torch.moveaxis(prox_weight_nw * flow12_mask, [0, 1, 2, 3], [0, 3, 1, 2])\n",
    "        weight_sw = torch.moveaxis(prox_weight_sw * flow12_mask, [0, 1, 2, 3], [0, 3, 1, 2])\n",
    "        weight_ne = torch.moveaxis(prox_weight_ne * flow12_mask, [0, 1, 2, 3], [0, 3, 1, 2])\n",
    "        weight_se = torch.moveaxis(prox_weight_se * flow12_mask, [0, 1, 2, 3], [0, 3, 1, 2])\n",
    "\n",
    "        frame2_offset = F.pad(frame2, [1, 1, 1, 1])\n",
    "        mask2_offset = F.pad(mask2, [1, 1, 1, 1])\n",
    "        bi = torch.arange(b)[:, None, None]\n",
    "\n",
    "        f2_nw = frame2_offset[bi, :, trans_pos_floor[:, 1], trans_pos_floor[:, 0]]\n",
    "        f2_sw = frame2_offset[bi, :, trans_pos_ceil[:, 1], trans_pos_floor[:, 0]]\n",
    "        f2_ne = frame2_offset[bi, :, trans_pos_floor[:, 1], trans_pos_ceil[:, 0]]\n",
    "        f2_se = frame2_offset[bi, :, trans_pos_ceil[:, 1], trans_pos_ceil[:, 0]]\n",
    "\n",
    "        m2_nw = mask2_offset[bi, :, trans_pos_floor[:, 1], trans_pos_floor[:, 0]]\n",
    "        m2_sw = mask2_offset[bi, :, trans_pos_ceil[:, 1], trans_pos_floor[:, 0]]\n",
    "        m2_ne = mask2_offset[bi, :, trans_pos_floor[:, 1], trans_pos_ceil[:, 0]]\n",
    "        m2_se = mask2_offset[bi, :, trans_pos_ceil[:, 1], trans_pos_ceil[:, 0]]\n",
    "\n",
    "        nr = weight_nw * f2_nw * m2_nw + weight_sw * f2_sw * m2_sw + \\\n",
    "             weight_ne * f2_ne * m2_ne + weight_se * f2_se * m2_se\n",
    "        dr = weight_nw * m2_nw + weight_sw * m2_sw + weight_ne * m2_ne + weight_se * m2_se\n",
    "\n",
    "        zero_value = -1 if is_image else 0\n",
    "        zero_tensor = torch.tensor(zero_value, dtype=nr.dtype, device=nr.device)\n",
    "        warped_frame1 = torch.where(dr > 0, nr / dr, zero_tensor)\n",
    "        mask1 = (dr > 0).to(frame2)\n",
    "\n",
    "        # Convert to channel first\n",
    "        warped_frame1 = torch.moveaxis(warped_frame1, [0, 1, 2, 3], [0, 2, 3, 1])\n",
    "        mask1 = torch.moveaxis(mask1, [0, 1, 2, 3], [0, 2, 3, 1])\n",
    "\n",
    "        if is_image:\n",
    "            assert warped_frame1.min() >= -1.1  # Allow for rounding errors\n",
    "            assert warped_frame1.max() <= 1.1\n",
    "            warped_frame1 = torch.clamp(warped_frame1, min=0, max=1)\n",
    "            \n",
    "        return warped_frame1, mask1\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def create_grid(batch, height, width, normalized_coordinates=False):\n",
    "        if normalized_coordinates:\n",
    "            xs = torch.linspace(-1, 1, width)\n",
    "            ys = torch.linspace(-1, 1, height)\n",
    "        else:\n",
    "            xs = torch.linspace(0, width - 1, width)\n",
    "            ys = torch.linspace(0, height - 1, height)\n",
    "#             x_1d = torch.arange(0, w)[None]\n",
    "#             y_1d = torch.arange(0, h)[:, None]\n",
    "#             x_2d = x_1d.repeat([h, 1])\n",
    "#             y_2d = y_1d.repeat([1, w])\n",
    "#             grid = torch.stack([x_2d, y_2d], dim=0)\n",
    "\n",
    "        base_grid = torch.stack(torch.meshgrid([xs, ys])).transpose(1, 2)  # 2xHxW\n",
    "        base_grid = base_grid[None]\n",
    "        \n",
    "#         print(base_grid.shape)\n",
    "        \n",
    "#             batch_grid = grid[None].repeat([b, 1, 1, 1])\n",
    "        \n",
    "        return base_grid\n",
    "\n",
    "    @staticmethod\n",
    "    def read_image(path: Path) -> torch.Tensor:\n",
    "        image = skimage.io.imread(path.as_posix())\n",
    "        return image\n",
    "\n",
    "    @staticmethod\n",
    "    def read_depth(path: Path) -> torch.Tensor:\n",
    "        if path.suffix == '.png':\n",
    "            depth = skimage.io.imread(path.as_posix())\n",
    "        elif path.suffix == '.npy':\n",
    "            depth = numpy.load(path.as_posix())\n",
    "        elif path.suffix == '.npz':\n",
    "            with numpy.load(path.as_posix()) as depth_data:\n",
    "                depth = depth_data['depth']\n",
    "        elif path.suffix == '.exr':\n",
    "            exr_file = OpenEXR.InputFile(path.as_posix())\n",
    "            raw_bytes = exr_file.channel('B', Imath.PixelType(Imath.PixelType.FLOAT))\n",
    "            depth_vector = numpy.frombuffer(raw_bytes, dtype=numpy.float32)\n",
    "            height = exr_file.header()['displayWindow'].max.y + 1 - exr_file.header()['displayWindow'].min.y\n",
    "            width = exr_file.header()['displayWindow'].max.x + 1 - exr_file.header()['displayWindow'].min.x\n",
    "            depth = numpy.reshape(depth_vector, (height, width))\n",
    "        else:\n",
    "            raise RuntimeError(f'Unknown depth format: {path.suffix}')\n",
    "        return depth\n",
    "\n",
    "    @staticmethod\n",
    "    def camera_intrinsic_transform(capture_width=1920, capture_height=1080, patch_start_point: tuple = (0, 0)):\n",
    "        start_y, start_x = patch_start_point\n",
    "        camera_intrinsics = numpy.eye(4)\n",
    "        camera_intrinsics[0, 0] = 2100\n",
    "        camera_intrinsics[0, 2] = capture_width / 2.0 - start_x\n",
    "        camera_intrinsics[1, 1] = 2100\n",
    "        camera_intrinsics[1, 2] = capture_height / 2.0 - start_y\n",
    "        return camera_intrinsics\n",
    "\n",
    "    @staticmethod\n",
    "    def get_device(device: str):\n",
    "        \"\"\"\n",
    "        Returns torch device object\n",
    "        :param device: cpu/gpu0/gpu1\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if device == 'cpu':\n",
    "            device = torch.device('cpu')\n",
    "        elif device.startswith('gpu') and torch.cuda.is_available():\n",
    "            gpu_num = int(device[3:])\n",
    "            device = torch.device(f'cuda:{gpu_num}')\n",
    "        else:\n",
    "            device = torch.device('cpu')\n",
    "        return device\n",
    "\n",
    "\n",
    "def demo1(frame1_path, depth1_path, transformation1, transformation2):\n",
    "    warper = Warper(device='gpu0')\n",
    "    frame1 = Image.open(frame1_path).convert(\"RGB\")\n",
    "    frame1 = (np.array(frame1) / 255.0).astype(np.float32)\n",
    "    frame1 = torch.tensor(frame1).permute(2, 0, 1).unsqueeze(0)\n",
    "    \n",
    "    depth1 = torch.from_numpy(warper.read_depth(depth1_path))[None, None, :, :]\n",
    "    mask1 = depth1 != torch.max(depth1)\n",
    "#     depth1[mask1] = (depth1[mask1] - torch.min(depth1[mask1])) / (torch.max(depth1[mask1]) - torch.min(depth1[mask1]))\n",
    "        \n",
    "    batch_size, _, height_src, width_src = frame1.shape\n",
    "    FOV = 60\n",
    "    focal = (height_src / 2.0) / np.tan(np.deg2rad(FOV / 2.0))\n",
    "    cx, cy = width_src // 2, height_src // 2\n",
    "    intrinsic = torch.FloatTensor([[[focal, 0,  cx],\n",
    "                                    [0, focal, cy],\n",
    "                                    [0,   0,    1]]])\n",
    "    intrinsic = intrinsic.expand(batch_size, -1, -1)\n",
    "\n",
    "    warped_frame2, mask2, warped_depth2, flow12 = warper.forward_warp(frame1, \n",
    "                                                                      mask1, \n",
    "                                                                      depth1, \n",
    "                                                                      transformation1, \n",
    "                                                                      transformation2, \n",
    "                                                                      intrinsic, \n",
    "                                                                      None,\n",
    "                                                                      render_method=\"splatting\")\n",
    "    \n",
    "    return frame1, warped_frame2, mask2, warped_depth2, flow12\n",
    "\n",
    "\n",
    "from packaging import version as pver\n",
    "\n",
    "def custom_meshgrid(*args):\n",
    "    # ref: https://pytorch.org/docs/stable/generated/torch.meshgrid.html?highlight=meshgrid#torch.meshgrid\n",
    "    if pver.parse(torch.__version__) < pver.parse('1.10'):\n",
    "        return torch.meshgrid(*args)\n",
    "    else:\n",
    "        return torch.meshgrid(*args, indexing='ij')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "496d22d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_pose2(radius=1, theta=None, phi=None, targets=0.0):\n",
    "    batch_size = theta.shape[0]\n",
    "    \n",
    "    theta = torch.deg2rad(theta)\n",
    "    phi = torch.deg2rad(phi)  \n",
    "            \n",
    "    centers = torch.stack([radius * torch.sin(theta) * torch.sin(phi),\n",
    "                           radius * torch.cos(theta),\n",
    "                           -radius * torch.sin(theta) * torch.cos(phi)], dim=-1) # [B, 3]\n",
    "        \n",
    "    # lookat\n",
    "    forward_vector = safe_normalize(targets - centers)\n",
    "    up_vector = torch.FloatTensor([0, 1, 0]).to(device).unsqueeze(0).repeat(batch_size, 1)\n",
    "    right_vector = safe_normalize(torch.cross(up_vector, forward_vector, dim=-1))\n",
    "    \n",
    "    up_vector = safe_normalize(torch.cross(forward_vector, right_vector, dim=-1))\n",
    "\n",
    "    poses = torch.eye(4, dtype=torch.float, device=device).unsqueeze(0).repeat(batch_size, 1, 1) # Original\n",
    "    poses[:, :3, :3] = torch.stack((right_vector, up_vector, forward_vector), dim=-1)\n",
    "    poses[:, :3, 3] = centers\n",
    "    \n",
    "    return poses\n",
    "    \n",
    "\n",
    "def safe_normalize(x, eps=1e-20):\n",
    "    return x / torch.sqrt(torch.clamp(torch.sum(x * x, -1, keepdim=True), min=eps))\n",
    "\n",
    "\n",
    "def inverse_mat(mat):\n",
    "    rmat = mat[:, :3, :3]\n",
    "    tvec = mat[:, :3, -1:]\n",
    "    rmat_inv = torch.transpose(rmat, 1, 2)\n",
    "    tvec_inv = rmat_inv.matmul(tvec)\n",
    "    \n",
    "    new_mat = torch.zeros_like(mat)\n",
    "    new_mat[:, :3, :3] = rmat\n",
    "    new_mat[:, :3, -1:] = tvec_inv\n",
    "    \n",
    "    return new_mat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84dbfcf0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m frame1_path \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/jh27kim/dataset/omniobject3d/OpenXD-OmniObject3D-New/raw/blender_renders/dinosaur/dinosaur_005/render/images/r_3.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m depth1_path \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/jh27kim/dataset/omniobject3d/OpenXD-OmniObject3D-New/raw/blender_renders/dinosaur/dinosaur_005/render/depths/r_3_depth.exr\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m phi_grid, theta_grid \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m20\u001b[39m]), np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m90\u001b[39m])\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# obj_center = torch.tensor([0.0450, -0.0436,  3.9582])\u001b[39;00m\n\u001b[1;32m      9\u001b[0m transformation1 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor([\n\u001b[1;32m     10\u001b[0m     [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m     11\u001b[0m     [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m     12\u001b[0m     [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m     13\u001b[0m     [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m     14\u001b[0m ])\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# Zoom in and out\n",
    "\n",
    "frame1_path = Path(\"/home/jh27kim/dataset/omniobject3d/OpenXD-OmniObject3D-New/raw/blender_renders/dinosaur/dinosaur_005/render/images/r_3.png\")\n",
    "depth1_path = Path(\"/home/jh27kim/dataset/omniobject3d/OpenXD-OmniObject3D-New/raw/blender_renders/dinosaur/dinosaur_005/render/depths/r_3_depth.exr\")\n",
    "\n",
    "phi_grid, theta_grid = np.array([-20, -15, -10, -5, 0, 5, 10, 15, 20]), np.array([90])\n",
    "\n",
    "# obj_center = torch.tensor([0.0450, -0.0436,  3.9582])\n",
    "transformation1 = torch.FloatTensor([\n",
    "    [1, 0, 0, 0],\n",
    "    [0, 1, 0, 0],\n",
    "    [0, 0, 1, -1],\n",
    "    [0, 0, 0, 1],\n",
    "]).unsqueeze(0)\n",
    "rotated_img = []\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "for phi_dl in phi_grid:\n",
    "    for theta_dl in theta_grid:        \n",
    "        transformation2 = rand_pose2(theta=torch.FloatTensor([theta_dl]).to(device), \n",
    "                                    phi=torch.FloatTensor([phi_dl]).to(device))\n",
    "        \n",
    "\n",
    "        frame1, warped_frame2, mask2, warped_depth2, flow12 = demo1(frame1_path, \n",
    "                                                                    depth1_path,\n",
    "                                                                    transformation1, \n",
    "                                                                    transformation2)\n",
    "        rotated_img.append(warped_frame2)\n",
    "        print(\"phi\", phi_dl)\n",
    "        print(\"theta\", theta_dl)\n",
    "        print(\"transformation2\", transformation2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ffd861",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 3, figsize=(16, 16))\n",
    "\n",
    "axs = axs.flatten()\n",
    "for i, (ax, img) in enumerate(zip(axs, rotated_img)):\n",
    "    ax.imshow(torch_to_pil(img))\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_title(f\"rotation: phi {phi_grid[i]} theta {theta_grid[i % theta_grid.shape[0]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106ca152",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Image.open(\"/home/jh27kim/dataset/omniobject3d/OpenXD-OmniObject3D-New/raw/blender_renders/dinosaur/dinosaur_005/render/images/r_3.png\").convert(\"RGB\")\n",
    "np.array(a)[400][400], np.array(a)[450][450], np.array(a)[450][400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e11fb766",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[[ 9.3969e-01, -1.4950e-08,  3.4202e-01, -3.4202e-01],\n",
    "                 [ 0.0000e+00,  1.0000e+00,  4.3711e-08, -4.3711e-08],\n",
    "                 [-3.4202e-01, -4.1075e-08,  9.3969e-01, -9.3969e-01],\n",
    "                 [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d173844d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 9.3969e-01, -1.4950e-08,  3.4202e-01,  0.0000e+00],\n",
      "         [ 0.0000e+00,  1.0000e+00,  4.3711e-08,  0.0000e+00],\n",
      "         [-3.4202e-01, -4.1075e-08,  9.3969e-01,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]]) tensor([[[1., 0., 0., -0.],\n",
      "         [0., 1., 0., -0.],\n",
      "         [0., 0., 1., 1.],\n",
      "         [0., 0., 0., 1.]]])\n",
      "torch.Size([4, 1]) torch.Size([4, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[3.6685e-01],\n",
       "         [1.9845e-06],\n",
       "         [1.0079e+00],\n",
       "         [1.0000e+00]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T1 = torch.FloatTensor([\n",
    "    [1, 0, 0, 0],\n",
    "    [0, 1, 0, 0],\n",
    "    [0, 0, 1, -1],\n",
    "    [0, 0, 0, 1],\n",
    "]).unsqueeze(0)\n",
    "\n",
    "T2 = torch.tensor([[[ 9.3969e-01, -1.4950e-08,  3.4202e-01, -3.4202e-01],\n",
    "         [ 0.0000e+00,  1.0000e+00,  4.3711e-08, -4.3711e-08],\n",
    "         [-3.4202e-01, -4.1075e-08,  9.3969e-01, -9.3969e-01],\n",
    "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]]])\n",
    "\n",
    "ans = torch.tensor([[ 9.3969e-01, -1.4950e-08,  3.4202e-01,  2.9802e-08],\n",
    "                    [ 0.0000e+00,  1.0000e+00,  4.3711e-08,  3.5527e-15],\n",
    "                    [-3.4202e-01, -4.1075e-08,  9.3969e-01,  5.9605e-08],\n",
    "                    [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  1.0000e+00]],)\n",
    "\n",
    "print(torch.bmm(T2, torch.linalg.inv(T1)), torch.linalg.inv(T1))\n",
    "sample_pts = torch.tensor([[1.9376e-06], \n",
    "                           [1.9376e-06],\n",
    "                           [1.0726e+00],\n",
    "                           [1.0000e+00]])\n",
    "\n",
    "print(sample_pts.shape, ans.shape)\n",
    "ans_pts = torch.tensor([[3.6686e-01],\n",
    "                       [1.9845e-06],\n",
    "                       [1.0079e+00],\n",
    "                       [1.0000e+00]])\n",
    "torch.matmul(torch.bmm(T2, torch.linalg.inv(T1)), sample_pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebac35d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
